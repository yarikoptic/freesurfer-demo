/**
 * @file  mrivol2vol_cuda.cu
 * @brief Holds MRI volume to volume transform for CUDA
 *
 * Implements MRIvol2vol for the GPU
 */
/*
 * Original Author: Richard Edgar
 * CVS Revision Info:
 *    $Author: nicks $
 *    $Date: 2010/10/22 22:40:14 $
 *    $Revision: 1.20.2.1 $
 *
 * Copyright (C) 2002-2008,
 * The General Hospital Corporation (Boston, MA). 
 * All rights reserved.
 *
 * Distribution, usage and copying of this software is covered under the
 * terms found in the License Agreement file named 'COPYING' found in the
 * FreeSurfer source code root directory, and duplicated here:
 * https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferOpenSourceLicense
 *
 * General inquiries: freesurfer@nmr.mgh.harvard.edu
 * Bug reports: analysis-bugs@nmr.mgh.harvard.edu
 *
 */

#include <cstdlib>
#include <cstdio>

#include <iostream>
#include <iomanip>


#include "chronometer.hpp"
#include "cudacheck.h"


#include "mriframegpu.hpp"
#include "affinegpu.hpp"

#include "mrivol2vol_cuda.h"
#include "mrivol2vol_cuda.hpp"

// ==============================================================

//! Texture reference for an unsigned char source
texture<unsigned char, 3, cudaReadModeNormalizedFloat> dt_src_uchar;

//! Texture reference for a short source
texture<short, 3, cudaReadModeNormalizedFloat> dt_src_short;

//! Texture reference for a float source
texture<float, 3, cudaReadModeElementType> dt_src_float;


namespace GPU {
  namespace Algorithms {

    const unsigned int kVol2VolBlockSize = 16;

    // ---------------------------------------


    template<>
    void MRIvol2vol::BindSourceTexture<unsigned char>( const GPU::Classes::MRIframeGPU<unsigned char>& src,
					   const int InterpMode ) {
      /*!
	Binds the unsigned char texture
      */

      dt_src_uchar.normalized = false;
      dt_src_uchar.addressMode[0] = cudaAddressModeClamp;
      dt_src_uchar.addressMode[1] = cudaAddressModeClamp;
      dt_src_uchar.addressMode[2] = cudaAddressModeClamp;

      // Set the interpolation
      switch( InterpMode ) {
      case SAMPLE_NEAREST:
	dt_src_uchar.filterMode = cudaFilterModePoint;
	break;

      case SAMPLE_TRILINEAR:
	dt_src_uchar.filterMode = cudaFilterModeLinear;
	break;

      default:
	std::cerr << __FUNCTION__ << ": Unrecognised InterpMode "
		  << InterpMode << std::endl;
	exit( EXIT_FAILURE );
      }

      CUDA_SAFE_CALL( cudaBindTextureToArray( dt_src_uchar, src.GetArray() ) );

    }

    
    template<>
    void MRIvol2vol::BindSourceTexture<float>( const GPU::Classes::MRIframeGPU<float>& src,
				   const int InterpMode ) {
      /*!
	Binds the float texture
      */

      dt_src_float.normalized = false;
      dt_src_float.addressMode[0] = cudaAddressModeClamp;
      dt_src_float.addressMode[1] = cudaAddressModeClamp;
      dt_src_float.addressMode[2] = cudaAddressModeClamp;

      // Set the interpolation
      switch( InterpMode ) {
      case SAMPLE_NEAREST:
	dt_src_float.filterMode = cudaFilterModePoint;
	break;

      case SAMPLE_TRILINEAR:
	dt_src_float.filterMode = cudaFilterModeLinear;
	break;

      default:
	std::cerr << __FUNCTION__ << ": Unrecognised InterpMode "
		  << InterpMode << std::endl;
	exit( EXIT_FAILURE );
      }

      CUDA_SAFE_CALL( cudaBindTextureToArray( dt_src_float, src.GetArray() ) );

    }

    template<>
    void MRIvol2vol::BindSourceTexture<short>( const GPU::Classes::MRIframeGPU<short>& src,
				   const int InterpMode ) {
      /*!
	Binds the short texture
      */

      dt_src_short.normalized = false;
      dt_src_short.addressMode[0] = cudaAddressModeClamp;
      dt_src_short.addressMode[1] = cudaAddressModeClamp;
      dt_src_short.addressMode[2] = cudaAddressModeClamp;

      // Set the interpolation
      switch( InterpMode ) {
      case SAMPLE_NEAREST:
	dt_src_short.filterMode = cudaFilterModePoint;
	break;

      case SAMPLE_TRILINEAR:
	dt_src_short.filterMode = cudaFilterModeLinear;
	break;

      default:
	std::cerr << __FUNCTION__ << ": Unrecognised InterpMode "
		  << InterpMode << std::endl;
	exit( EXIT_FAILURE );
      }

      CUDA_SAFE_CALL( cudaBindTextureToArray( dt_src_short, src.GetArray() ) );

    }


    // ---------------------------------------

    

    template<> void MRIvol2vol::UnbindSourceTexture<unsigned char>( void ) {
      CUDA_SAFE_CALL( cudaUnbindTexture( dt_src_uchar ) );
    }

    template<> void MRIvol2vol::UnbindSourceTexture<float>( void ) {
      CUDA_SAFE_CALL( cudaUnbindTexture( dt_src_float ) );
    }

    template<> void MRIvol2vol::UnbindSourceTexture<short>( void ) {
      CUDA_SAFE_CALL( cudaUnbindTexture( dt_src_short ) );
    }

    // ---------------------------------------

    //! Templated texture fetch
    template<typename T>
    __device__ float FetchSrcVoxel( const float3 r ) {
      /*!
	Does look ups into the textures.
	Recall that the textures are configured to return
	normalised floats (except for the float texture!)
	in order to enable linear filtering.
	This in turn requires a conversion from the normalised
	value back to the true value.
	Unspecialised version writes junk
      */

      return( 100000 );
    }

    template<>
    __device__ float FetchSrcVoxel<unsigned char>( const float3 r ) {

      float texVal;
      
      // Do the fetch, remembering texel centre offset
      texVal = tex3D( dt_src_uchar, r.x+0.5f, r.y+0.5f, r.z+0.5f );

      // We configured to return a normalised float, so get back in range
      texVal *= UCHAR_MAX;

      return( texVal );
    }

    template<>
    __device__ float FetchSrcVoxel<short>( const float3 r ) {

      float texVal;
      
      // Do the fetch, remembering texel centre offset
      texVal = tex3D( dt_src_short, r.x+0.5f, r.y+0.5f, r.z+0.5f );

      // We configured to return a normalised float, so get back in range
      texVal *= SHRT_MAX;

      return( texVal );
    }
    

    template<>
    __device__ float FetchSrcVoxel<float>( const float3 r ) {
      // Do the fetch, remembering texel centre offset
      float texVal;
      
      texVal = tex3D( dt_src_float, r.x+0.5f, r.y+0.5f, r.z+0.5f );

      // No need to convert - float texture read mode was ElementType
      return( texVal );
    }

    // ---------------------------------------


    //! Kernel to perform the affine transformation
    template<typename T, typename U>
    __global__ void MRIVol2VolKernel( GPU::Classes::MRIframeOnGPU<U> dst,
				      GPU::Classes::AffineTransformation afTrans,
				      const dim3 coverGrid ) {
      /*!
	Kernel to perform the affine transformation supplied on the
	MRI frame bound to the textures.
	Stores the result in the dst frame.
	Each thread produces one element of the result.
	The z co-ordinate will be in blockIdx.y
	The (x,y) location of the patch will be derived from blockIdx.x.
	Since the slices array is padded out to multiples of 16 by
	the other MRI slices routines (the host routine checks this), we
	don't have to worry about nasty edge cases
      */

      // Extract some co-ordinates
      const unsigned int by = blockIdx.x / coverGrid.x;
      const unsigned int bx = blockIdx.x % coverGrid.x;
      const unsigned int iz = blockIdx.y;
      const unsigned int tx = threadIdx.x;
      const unsigned int ty = threadIdx.y;

      const unsigned int ixStart = bx * kVol2VolBlockSize;
      const unsigned int iyStart = by * kVol2VolBlockSize;
      const unsigned int myx = ixStart + tx;
      const unsigned int myy = iyStart + ty;

      float3 rOut = afTrans.transform( make_float3( myx, myy, iz ) );

      float res;

      const float tol = 0.5f;

      if( dst.InFuzzyVolume( rOut, tol ) ) {
	res = FetchSrcVoxel<T>( rOut );
      } else {
	res = 0;
      }

      if( dst.InVolume( myx, myy, iz ) ) {
	dst( myx, myy, iz ) = dst.ConvertFloat( res );
      }
    }


    template<typename T, typename U>
    void MRIvol2vol::transformGPU( const GPU::Classes::MRIframeGPU<T> &src,
				   GPU::Classes::MRIframeGPU<U> &dst,
				   const GPU::Classes::AffineTransformation& transform,
				   const int InterpMode,
				   const cudaStream_t myStream ) {
      /*!
	This is the dispatch function for the vol2vol kernel.
	The src MRI must already be in its CUDA array, and the
	dst MRI must be padded
      */

      // Run through a couple of sanity checks
      if( !src.HasArray() ) {
	std::cerr << __FUNCTION__ <<
	  ": Source array must be in CUDA array" << std::endl;
	exit( EXIT_FAILURE );
      }

      
      // Bind the texture and array
      this->BindSourceTexture( src, InterpMode );


      const dim3 dstDims = dst.GetDims();
      dim3 grid, threads;

      threads.x = threads.y = kVol2VolBlockSize;
      threads.z = 1;

      const dim3 coverGrid = dst.CoverBlocks( kVol2VolBlockSize );

      grid.x = coverGrid.x * coverGrid.y;
      grid.y = dstDims.z;
      grid.z = 1;

      // Main dispatch
      switch( InterpMode ) {

      case SAMPLE_NEAREST:
      case SAMPLE_TRILINEAR:
	MRIVol2VolKernel<T,U>
	  <<<grid,threads>>>( dst, transform, coverGrid );
	CUDA_CHECK_ERROR_ASYNC( "MRIVol2VolKernel call failed!\n" );
	break;

      case SAMPLE_SINC:
	std::cerr << __FUNCTION__ << ": Sinc interpolation not yet supported"
		  << std::endl;
	exit( EXIT_FAILURE );
      }


      // Unbind the texture
      this->UnbindSourceTexture<T>();
    }  


    //! Routine to use GPU for vol2vol on all MRI frames
    template<typename T, typename U>
    void MRIvol2vol::AllFramesGPU( const MRI* src, MRI* targ,
				   const MATRIX* transformMatrix,
				   const int InterpMode,
				   const float param ) {
      
      tVol2VolTotal.Start();

      // Get hold of the affine transformation
      GPU::Classes::AffineTransformation myTransform( transformMatrix );

      GPU::Classes::MRIframeGPU<T> srcGPU;
      GPU::Classes::MRIframeGPU<U> dstGPU;

      char* h_workspace;
      size_t srcWorkSize, dstWorkSize;

      // Allocate GPU arrays
      tVol2VolMem.Start();
      srcGPU.Allocate( src );
      srcGPU.AllocateArray();
      dstGPU.Allocate( targ );
      tVol2VolMem.Stop();

      // Sanity check
      srcGPU.VerifyMRI( src );
      dstGPU.VerifyMRI( targ );

      // Allocate workspace array
      tVol2VolMemHost.Start();
      srcWorkSize = srcGPU.BufferSize();
      dstWorkSize = dstGPU.BufferSize();
      
      if( srcWorkSize > dstWorkSize ) {
	CUDA_SAFE_CALL( cudaHostAlloc( (void**)&h_workspace,
				       srcWorkSize,
				       cudaHostAllocDefault ) );
      } else {
	CUDA_SAFE_CALL( cudaHostAlloc( (void**)&h_workspace,
				       dstWorkSize,
				       cudaHostAllocDefault ) );
      }
      tVol2VolMemHost.Stop();

      // Loop over frames
      for( unsigned int iFrame=0; iFrame < src->nframes; iFrame++ ) {
	// Get the next source frame
	tVol2VolMRISendFrame.Start();
	srcGPU.SendFrame( src, iFrame, h_workspace );
	tVol2VolMRISendFrame.Stop();
	
	// Put it into a CUDA array
	tVol2VolMRISendArray.Start();
	srcGPU.SendArray();
	tVol2VolMRISendArray.Stop();

	// Run the convolution
	tVol2VolCompute.Start();
	this->transformGPU( srcGPU, dstGPU, myTransform, InterpMode );
	tVol2VolCompute.Stop();

	// Get the results back
	tVol2VolMRIRecvFrame.Start();
	dstGPU.RecvFrame( targ, iFrame, h_workspace );
	tVol2VolMRIRecvFrame.Stop();
      }

      CUDA_CHECK_ERROR( "MRI Vol2Vol failed on GPU" );

      // Release workspace array
      tVol2VolMemHost.Start();
      CUDA_SAFE_CALL( cudaFreeHost( h_workspace ) );
      tVol2VolMemHost.Stop();

      // No need to release MRIframeGPU types - destructors will handle it
      tVol2VolTotal.Stop();
    }


    //! Dispatch routine to add templating
    template<typename T>
    void MRIvol2vol::AllFramesDstDispatch( const MRI* src, MRI* targ,
					   const MATRIX* transformMatrix,
					   const int InterpMode,
					   const float param ) {
      /*!
	A thin wrapper to produce the correct template
	based on the type of the target MRI
      */

      switch( targ->type ) {
      case MRI_UCHAR:
	this->AllFramesGPU<T,unsigned char>( src, targ,
					     transformMatrix,
					     InterpMode,
					     param );
	break;

      case MRI_FLOAT:
	this->AllFramesGPU<T,float>( src, targ,
				     transformMatrix,
				     InterpMode,
				     param );
	break;

      case MRI_SHORT:
	this->AllFramesGPU<T,short>( src, targ,
				     transformMatrix,
				     InterpMode,
				     param );
	break;

      default:
	std::cerr << __FUNCTION__ << ": Unrecognised target type "
		  << targ->type << std::endl;
	exit( EXIT_FAILURE );
      }
    }

    //! Dispatch routine on src MRI
    void MRIvol2vol::transform( const MRI* src, MRI* targ,
				const MATRIX* transformMatrix,
				const int InterpMode,
				const float param ) {
      
      switch( src->type ) {
      case MRI_UCHAR:
	this->AllFramesDstDispatch<unsigned char>( src, targ,
						   transformMatrix,
						   InterpMode,
						   param );
	break;

      case MRI_FLOAT:
	this->AllFramesDstDispatch<float>( src, targ,
					   transformMatrix,
					   InterpMode,
					   param );
	break;

      case MRI_SHORT:
	this->AllFramesDstDispatch<short>( src, targ,
					   transformMatrix,
					   InterpMode,
					   param );
	break;
	
	
      default:
	std::cerr << __FUNCTION__ << ": Unrecognised src data type "
		  << src->type << std::endl;
	exit( EXIT_FAILURE );
      }
    }


    // ===================================================================

    // Define the static members
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolMem;
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolMemHost;
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolMRISendFrame;
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolMRIRecvFrame;
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolMRISendArray;
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolCompute;
    SciGPU::Utilities::Chronometer MRIvol2vol::tVol2VolTotal;


    void MRIvol2vol::ShowTimings( void ) {
      std::cout << "=============================================" << std::endl;
      std::cout << "GPU Vol2Vol timers" << std::endl;
      std::cout << "------------------" << std::endl;
#ifndef CUDA_FORCE_SYNC
      std::cout << "WARNING: CUDA_FORCE_SYNC not #defined" << std::endl;
      std::cout << "Timings may not be accurate" << std::endl;
#endif
      std::cout << std::endl;
      
      std::cout << "MRIVol2VolAllFramesGPU" << std::endl;
      std::cout << "Host Memory : " << MRIvol2vol::tVol2VolMemHost << std::endl;
      std::cout << "GPU Memory  : " << MRIvol2vol::tVol2VolMem << std::endl;
      std::cout << "Send Frame  : " << MRIvol2vol::tVol2VolMRISendFrame << std::endl;
      std::cout << "Send Array  : " << MRIvol2vol::tVol2VolMRISendArray << std::endl;
      std::cout << "Compute     : " << MRIvol2vol::tVol2VolCompute << std::endl;
      std::cout << "Recv Frame  : " << MRIvol2vol::tVol2VolMRIRecvFrame << std::endl;
      std::cout << "---------------------" << std::endl;
      std::cout << "Total : " << MRIvol2vol::tVol2VolTotal << std::endl;
      
      std::cout << "=============================================" << std::endl;
    }
  }
}


// ======================================================

static GPU::Algorithms::MRIvol2vol myvol2vol;


int MRIvol2vol_cuda( const MRI* src, MRI* targ, 
		     const MATRIX* transformMatrix,
		     const int InterpMode,
		     const float param ) {
  /*!
    Reimplementation of MRIvol2vol for the GPU.
    Is meant to be called from within MRIvol2vol,
    once that routine has performed necessary checks
  */

  myvol2vol.transform( src, targ, transformMatrix, InterpMode, param );


  return( 0 );
}
    


